{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet name: Spring-2023\n",
      "Sheet name: Fall-2022\n",
      "Sheet name: Spring-2022\n",
      "Sheet name: Fall-2021\n",
      "Sheet name: Spring-2021\n",
      "Sheet name: Fall-2020\n",
      "Sheet name: Spring-2020\n",
      "Sheet name: Fall-2019\n",
      "Sheet name: Spring-2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mustf\\anaconda3\\envs\\py310\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "semester = list()\n",
    "file_path = 'MS_Thesis_Repository_FSC.xlsx'\n",
    "all_data = pd.read_excel(file_path, sheet_name=None)\n",
    "thesis_data = all_data\n",
    "for sheet_name, data in all_data.items():\n",
    "    print(\"Sheet name:\", sheet_name)\n",
    "    semester.append(sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mustf\\anaconda3\\envs\\py310\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'MS_Thesis_Repository_FSC.xlsx'\n",
    "all_data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "for sheet_name, data in all_data.items():\n",
    "    processed_data = data.iloc[1:, [1, 3]]  \n",
    "    processed_data.columns = ['Thesis Title', 'Thesis Abstract']\n",
    "    \n",
    "    output_path = f'{sheet_name}.xlsx'\n",
    "    processed_data.to_excel(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been combined and saved to combined_output.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# List to hold data from each Excel file\n",
    "dataframes = []\n",
    "\n",
    "for file,data in all_data.items():\n",
    "    # Read the Excel file into a DataFrame\n",
    "    file = f'{file}.xlsx'\n",
    "    df = pd.read_excel(file)\n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Path for the resulting text file\n",
    "txt_file_path = 'combined_output.txt'\n",
    "\n",
    "# Write the combined DataFrame to a text file, separated by tabs\n",
    "combined_df.to_csv(txt_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"All files have been combined and saved to {txt_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import tiktoken\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "path_f = 'combined_output.txt'\n",
    "\n",
    "loader = TextLoader(path_f)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=7000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "page_content='Thesis Title\\tThesis Abstract\\n\"21I-2191 Laraib Afzal\\n\\nProsodic alignment for Automatic dubbing\\n\\n\"\\tAutomatic dubbing is the process of replacing the audio track of a video with a different language. In automatic dubbing, prosodic alignment is used to match the suprasegmental features like timbre, prosody, duration, pauses and intonation of the original speech with synthesed speech, in order to produce a natural-sounding dubbed video. This is done by analyzing and mapping these features of the original and translated speech. Existing research on automatic dubbing lack to addresses these features in source video which impact the overall naturalness and fluency of Synthesized speech. To solve this we proposed end-to-end architecture, following modular approach, to generate high quality dubbed video. In this research, we mainly focus on TTS module by considering mentioned features for prosodic alignment in generated speech with synchronization of original utterance. We train our model learn to predict the suprasegmental features of the input voice and generate synthesized speech that matches the pattern of stress and intonation of the input voice. This help our proposed model to create a more natural and coherent output and improve the overall accuracy of end to end architecture. We explore auto-regressive decoder models and Constructive voice-voice pre-training model for our TTS module. We used a common voice English dataset which contains English text and corresponding voices. To evaluate our model we perform Quantitative Evaluation by calculating Mel Cepstral Distortion (MCD) with Dynamic Time Warping (DTW) and Word Error Rate.\\n\"21I-2195 Ansa Liaqat\\n\\n\\nBoundary-Aware Multiscale Brain Tumor Segmentation \\nwith a Compact Model using Spatial Pyramid Pooling\"\\tOne of the most difficult tasks in medical image analysis is brain tumor segmentation. To accurately delineate the regions of brain tumors is the aim of brain tumor segmentation. Deep learning techniques have recently shown promising results in resolving a variety of computer vision problems, including semantic segmentation, object detection, and image classification. Many deep learning-based techniques have been used to segment brain tumors, and the results are encouraging. Existing studies propose solutions for brain tumor segmentation with large computing resources. Our goal is to minimize computational time. We first segment the tumor region on the basis of which we crop the images. Then sent to networks for accurate and fast segmentation of sub-regions.\\n\"21I-2225 Maryam Shahbaz\\n\\n\\nLeukemia Type Classification and Identification of Cause-\\nEffect Relationship on Clinical Datasets\"\\t\"Data related to leukemia was collected from 209 countries and territories in 2019 by [1], which identified it as the most common cancer among children. Therefore, it is crucial to adopt a lifestyle that can help prevent leukemia. Although similar research has been conducted previously,but it has not been carried out specifically in Pakistan. People may have different lifestyles due to varying regions. In this study, we aimed to identify the lifestyle and demographic factors associated with the development of leukemia and predict the type of leukemia using clinical data. We collected data from a sample of 1258 individuals from IBGE Islamabad, Pakistan, which was categorized into laboratory, demographic, and lifestyle data.To analyze the demographic and lifestyle data, XGBoost, odd ratios, and CDR are utilized to predict the risk of leukemia. Our findings suggest that passive smoking, residing in rural areas, exposure to a polluted environment, poor nutrition, and lack of perfume usage increase the likelihood of developing leukemia. Similarly, from demographic perspective, children are more vulnerable to leukemia. Furthermore, we\\ntransformed the complete clinical data into neo4j graph data and utilized it to classify the type of leukemia. Comparing the accuracy of our classification using both graph and tabular data, tabular data demonstrated higher accuracy, achieving a 96% accuracy rate on over-sampled data using random forest algorithm.\"\\n\"21I-2313 Khadija Mahmood\\n\\n\\nImprovement of deep learning model for Human Pose Estimation\"\\tHuman Pose Estimation has gained high prominence in the computer vision domain because of its wide variety of demands in real-world applications. Fast inference time for estimating human poses is highly required such as action recognition of humans by robots for responding, human-computer interaction, and augmented reality. However, Existing State-of-the-art Multi-Person pose estimation methods require heavy computational resources for accurate predictions sacrificing Time Efficiency due to utilizing over-parameterized models. In this work, a compressed architecture based on deep neural networks for gaining time and resource efficiency has been proposed named: Distill Transpose. Distill Transpose consists of three modules feature extractor, distill encoder, and head. Feature extractor utilizes MobileNet network with the minor modification of removing the downsampling layer outputs high-resolution feature maps. Distill encoder is the compressed structure of the transformer’s encoder in which pruning-based methods have been used for the compression generating heat maps. The Final Head Module is a post-processing method used for refining the heat maps, giving the final predicted human body key points. The benchmark datasets: COCO and MPII human pose shall be used for this work\\n\"20I-2014 - Mutahher Mahmood' metadata={'source': 'combined_output.txt'}\n",
      "[+]=====[+]\n",
      "[+]Done --> Splitting txt file to chunks\n"
     ]
    }
   ],
   "source": [
    "print(type(texts))\n",
    "print(texts[0])\n",
    "print(\"[+]=====[+]\")\n",
    "print(\"[+]Done --> Splitting txt file to chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+]=====[+]\n",
      "Number Of Token: 139703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-15 07:47:40.192\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mfastembed.embedding\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mDefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated.Use from fastembed import TextEmbedding instead.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bf7c38ab3949ff93fb1d5a6269dbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(path_f, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "    \n",
    "print(\"[+]=====[+]\")\n",
    "numtoken = num_tokens_from_string(content, \"cl100k_base\")\n",
    "print(\"Number Of Token: \" + str(numtoken))\n",
    "\n",
    "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\", max_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+]=====[+]\n",
      "[+]Done  --> Embedding the Chunks, and saving to a vectorstore\n",
      "Ramsha Amin (MS-CS)\n",
      "Supervisor: Dr. Amna Basharat \"\tSemantic web is an extension of the current web in which semantic-based techniques are used to define and link data which provides better information integration and search experiences for the web users. In the domain of Islamic knowledge, especially focusing on the Quranic Tajweed which is the science of Quranic recitation, not enough work has been done for gathering all the knowledge related to the recitation rules of the Holy Quran in one place. Ontologies, are one of the tools frequently used for organizing knowledge which describes the terms of the domain and explains the relationships among them. In our study, we have developed an ontology-based Tajweed knowledge model by using Proteg ́ e framework and state-of-the-art semantic web technologies (OWL and SPARQL). METHONTOLOGY, an iterative design methodology was used for the ontology development that describes the articulation points of Arabic letters and their characteristics together with the Tajweed rules. It consists of 47 concepts and includes 167 individuals in Arabic, English, and Urdu. Semantic Web Rule Language (SWRL) was used for the implementation of the Tajweed rules. The ontology was evaluated in two steps. In the first phase the expert driven validation and criteria based evaluation was conducted for the Arabic letters and their charac teristics to evaluate the accuracy and structure of ontology. Results from the experts were incrementally improved before evaluating it with the next expert which results in 100% accuracy. In the second stage we evaluated the developed Tajweed rules on the complete text of the Holy Quran. We achieved almost the best F1 score i.e, 94% which shows that maximum rules were predicted correctly by our designed knowledge model. Also, the dataset of the entire Quran was generated in OWL format using the developed ontology.\n",
      "\"ML based Classification to Predict Aetiological LncRNAs in Humans \n",
      "\n",
      "Razia Khalid  (MS-CS)\n",
      "Supervisor: Dr. Zoya Khalid\"\t\"Long non-coding RNAs (lncRNAs), which were once considered as transcriptional to fully comprehend the involvement of lncRNAs with disease formation.\n",
      "noise, are now in the limelight of current research. LncRNAs play a major role in regulating various biological processes such as imprinting, cell differentiation, and splicing, to name a few. The mutations of lncRNAs are involved in various complex diseases and identifying lncRNA-disease associations has gained a lot of attention as predicting it efficiently will lead towards better disease treatment. In this study, we have developed a machine learning model that predicts disease-related lncRNAs by combining sequence and structure-based features. The features were trained on different machine learning models that includes SVM, Naive Bayes, Xgboost and Random Forest. The results showed that SVM had outperformed with the highest F1-score of 0.75 without similarity cut-off and 0.55 with a 70% similarity cut-off. We compared our method with the state-of-the-art and obtained a similar F1-score of 0.65 with non-redundant data and 0.86 with redundant data. Moreover, this study has overcome two serious limitations of the reported method which are lack in redundancy checking and implementation of oversampling for balancing the positive and negative classes. So far, our developed method has achieved an improved performance among the machine learn ing models reported for the lncRNA-disease associations. Still, there is a huge room for improvement, hence in the future, we will also combine features from biological pathways to predict lncRNA disease association and\"\n",
      "\"Predicting Drug-Target Interactions Using ML Techniques \n",
      "\n",
      "Hamna Anwar (MS-CS)\n",
      "Supervisor: Dr. Zoya Khalid \"\tThe traditional drug development process is long and costly, with drug side effects often being the cause of its high failure rate. Side effects occur because drugs typically bind to multiple protein targets in the body. We aim to supplement this development process by computationaly predicting drug target interactions, thereby saving time and money as well as enabling drug repuurposing. We leverage sequence features of the targets and in some models, molecular structural features of drugs, as predictors for various machine learning algorithms. Sequence information is easily available for many more targets than other complex feature information such as structural information and gene expression data. Models are created for two broad categories of targets: those belonging to diverse protein families and those belonging to the kinase protein Results show that SVM with protein and drug features performs best on both the Kifamily. nase and Diverse datasets. On the Kinase dataset, we achieved 60.7% sensitivity and 61.9% Specificity. The state of the art method [1], which uses structural binding site information, achieves an average sensitivity of 52% and average specificity of 55%. On the Diverse dataset, we achieved 98.2% sensitivity and 98.2% specificity. In comparison, the state of the art method, achieves an average sensitivity of 63% and average specificity of 81%. Therefore, we can use SVM with features consisting of simple sequence information of targets and molecular structural information of drugs as a generic model for classifying drug-target predictions of targets and drugs belonging to same and diverse families\n",
      "\"Detecting Web Applications Vulnerabilities Using Machine Learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    path=\"./local_qdrant1\",\n",
    "    collection_name=\"db2\",\n",
    ")\n",
    "print(\"[+]=====[+]\")\n",
    "print(\"[+]Done  --> Embedding the Chunks, and saving to a vectorstore\")\n",
    "\n",
    "question = input(\"Enter Any Question Related to the documents: \")\n",
    "docs = qdrant.similarity_search(question)\n",
    "\n",
    "print(docs[0].page_content)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastEmbedEmbeddings(model_name='BAAI/bge-base-en-v1.5', max_length=2048, cache_dir=None, threads=None, doc_embed_type='default', _model=<fastembed.text.text_embedding.TextEmbedding object at 0x000001E276BCA0E0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+]=====[+]\n",
      "[+]Done --> Preparing our chat model to make the similarity search result more human-like\n",
      "[+]=====[+]\n",
      "[+]Done --> Choosing the free llama3 7B for the task (Excellent)\n",
      "[+]=====[+]\n",
      "[+]Done --> Adding a retrieval QA\n",
      "[+]Go ahead and use the bot to answer any question from your documents (use ctrl + c to end the bot chat)\n",
      "Question: hello\n",
      "Answer ->>> \n",
      "Hello!\n",
      "\n",
      "It seems like you're looking for information related to the infrastructure less, flexible, self-configurable and is relatively small and less expense network. You also want to know more about efficient flexible periodic pattern mining, text classification using ensemble method, and predicting protein folding rates using machine learning.\n",
      "\n",
      "In regards to your question, I can provide some general insights on these topics:\n",
      "\n",
      "* Flying Ad-Hoc Networks (FANETs) are a type of infrastructure-less network that uses multiple Unmanned Air Vehicles (UAVs) to establish connectivity. FANETs have the potential to be used for various applications such as search and rescue operations.\n",
      "* Efficient flexible periodic pattern mining involves using novel properties to overcome limitations in existing algorithms. In this context, researchers Muhammad Fasih Javed and Dr. Kifayat Ullah proposed an Apriori-based approach combined with occurrence vectors to find flexible periodic patterns and reduce the time and space required for mining the patterns.\n",
      "* Text classification using ensemble methods is a technique that combines multiple machine learning models to improve the accuracy of text classification tasks. This approach can be useful in applications where high accuracy is critical, such as sentiment analysis or spam detection.\n",
      "\n",
      "In terms of predicting protein folding rates using machine learning, researchers Hannan Ali and Dr. Kashif Munir proposed a model that uses sequential and structural features to predict changes in protein folding rates. This approach has the potential to improve our understanding of protein folding dynamics and its implications for various diseases.\n",
      "\n",
      "Please let me know if you have any specific questions or if there's anything else I can help you with!\n",
      "\n",
      "Question: Human Action Recognition using Deep Learning\n",
      "Answer ->>> \n",
      "Based on the given context, it appears that you are planning to create a customized ensemble learner by combining ARIMA, XGBoost, and a modified LSTM model to improve the predictive power of the system. The primary objective is to achieve better results in terms of bias and variance.\n",
      "\n",
      "To answer your question: Yes, the modified LSTM is used to form the ensemble learner with other two models named as ARIMA and XGBoost.\n",
      "\n",
      "Question: Human Action Recognition using Deep Learning supervisor name\n",
      "Answer ->>> \n",
      "I didn't find any information related to \"Human Action Recognition using Deep Learning\" in the provided context. The text appears to be a collection of research papers or theses from students under the supervision of Dr. Hammad Majeed, with topics such as Reducing Entropy Over Estimation in SAC, Auto Improving Neural Architecture Search for Object Detection, Earthquake Prediction, and Anonymizing of Multi-Valued Stream Data Set.\n",
      "\n",
      "If you meant to ask about a different research paper or thesis, please provide more context or details, and I'll be happy to help.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparing our chat model to make the similarity search result more human-like\n",
    "template = \"\"\"[INST] <<SYS>> Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "<</SYS>>\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:[/INST]\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "print(\"[+]=====[+]\")\n",
    "print(\"[+]Done --> Preparing our chat model to make the similarity search result more human-like\")\n",
    "\n",
    "# Choosing the free llama 7B for the task (Excellent)\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    verbose=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")\n",
    "print(\"[+]=====[+]\")\n",
    "print(\"[+]Done --> Choosing the free llama3 7B for the task (Excellent)\")\n",
    "\n",
    "# Adding a retrieval QA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    chat_model,\n",
    "    retriever=qdrant.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "print(\"[+]=====[+]\")\n",
    "print(\"[+]Done --> Adding a retrieval QA\")\n",
    "print(\"[+]Go ahead and use the bot to answer any question from your documents (use ctrl + c to end the bot chat)\")\n",
    "\n",
    "# We are now ready, and good to go to ask question based on our document, and get human like answer\n",
    "while True:\n",
    "    \n",
    "    try:\n",
    "        question = input(\"Enter Prompt: \")\n",
    "        docs = qdrant.similarity_search(question)\n",
    "        print(\"Question: \" + question)\n",
    "        print(\"Answer ->>> \")\n",
    "        result = qa_chain({\"query\": question})\n",
    "        print(\"\\n\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"[+]==[+]\")\n",
    "        print(\"Bye! : Ending the Bot Chat\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
